## üìù First Place Solution in DSN Free AI Classes in Every City 2025

### üìå Overview and Objectives

This notebook presents a robust machine learning solution for predicting `Item_Store_Returns` in a retail forecasting challenge hosted on Zindi. The solution uses an ensemble of classical and tree-based models enhanced by pseudo-labeling techniques to improve generalization.

**Key Objectives:**

* Build a predictive regression model for `Item_Store_Returns`.
* Leverage polynomial feature expansion to capture non-linear patterns.
* Incorporate pseudo-labeling to enhance learning from the unlabeled test set.
* Combine predictions from diverse models via ensembling for improved accuracy.
* Produce a submission file adhering to domain constraints.

---

### üß© Architecture Diagram

```
+------------------+      +------------------+      +---------------------+
|   Raw Data       | -->  | Preprocessing &  | -->  | Polynomial Feature  |
| (train/test CSV) |      | Scaling (MinMax) |      | Transformation       |
+------------------+      +------------------+      +---------------------+
                                                             |
                                                             v
                          +----------------------+    +------------------------+
                          | Model Training       |    | Pseudo-Labeling Logic  |
                          | (Lasso, XGB, etc.)   |<-->| High-confidence filter |
                          +----------------------+    +------------------------+
                                      |                          |
                                      v                          v
                          +----------------------------------------------+
                          | Model Retraining on Original + Pseudo Data  |
                          +----------------------------------------------+
                                                  |
                                                  v
                          +--------------------------+
                          | Ensembling & Final Output |
                          +--------------------------+
```

---

### üõ†Ô∏è ETL Process

#### ‚úÖ Extract

* Data loaded from Zindi-provided CSV files: `train.csv` and `test.csv`.
* Format: tabular data with mixed numeric and categorical variables.

#### ‚úÖ Transform

* Applied **`MinMaxScaler`** to normalize all features between 0 and 1.
* Used **`PolynomialFeatures()`** for second-order interaction terms.
* Categorical features are label-encoded.

#### ‚úÖ Load

* Transformed data is fed directly into machine learning models.
* Outputs are used to pseudo-label test samples with low prediction variance across models.

---

### üß† Data Modeling

#### Models Used:

* **Lasso Regression** (L1 regularization)
* **Linear Regression**

#### Feature Engineering:

* Polynomial features from normalized numerical inputs.
* No feature selection‚Äîmodel regularization handles feature sparsity.
* Only kept a few features since others worsen the result.

#### Training Strategy:

* Single-stage training followed by pseudo-labeling.
* Models retrained on the original + high-confidence pseudo-labeled test samples.

#### Evaluation Metric:

* **Root Mean Squared Error (RMSE)** computed on training data as a proxy.
* Model predictions also manually post-processed for domain constraints.

---

### üöÄ Inference Pipeline

* Ensemble predictions are generated by **averaging predictions from six models**.
* Predictions are scaled down using a factor (`*0.975`), rounded to 2 decimal places.
* Final predictions are clipped to lie in the valid domain: **\[200, 15,000]**.
* A submission file is created: `sub.csv`.

---

### ‚è±Ô∏è Run Time (approximate)

| Step                         | Time Estimate |
| ---------------------------- | ------------- |
| Data loading & preprocessing | < 1 min 10 seconds      |
| Model training (all models)  | \~20 seconds  |
| Pseudo-labeling              | > ~2 second    |
| Retraining on pseudo data    | \~20 seconds  |
| Submission generation        | > 2 seconds   |
| Total                        | \~1m 51s      |
---

### üìà Performance Metrics

| Model          | Training RMSE (approx) |
| -------------- | ---------------------- |
| Lasso          | 2906.34                |
| Linear         | 2904.98                |
| XGBoost        | ‚úî                      |
| Bayesian Ridge | ‚úî                      |
| Huber          | ‚úî                      |
| ElasticNet     | ‚úî                      |

*Exact values are shown in the notebook. The final private/public leaderboard score should be added post-submission.*

Final Private Leaderboard: `2315.545979`

---

### ‚ö†Ô∏è Error Handling & Logging

* No formal logging, but assertions and clean preprocessing flow.
* Suggestions:

  * Add `try/except` blocks around model training/inference.
  * Log model metrics and confidence scores for pseudo-labeling.

---

### üîß Maintenance and Monitoring

**To scale or maintain this solution:**

* Periodically retrain models with new labeled data.
* Reassess pseudo-labeling confidence thresholds.
* Consider replacing hard-coded ensemble with a meta-model (e.g., stacking).
* Automate the pipeline using `Snakemake` or `MLflow` for production settings.

---

### ‚úÖ Best Practices Followed

* Reproducible: Uses fixed random seeds and standard libraries.
* Self-contained: No reliance on external pre-processed files or Excel steps.
* Fully scripted: Feature engineering, modeling, and inference all in-notebook.
* Domain-informed: Target capping and scaling mimic real-world constraints.

---

### üìé Environment Info

* Platform: Likely developed in a Kaggle Notebook or local Jupyter environment.
* Suggested addition: `requirements.txt` for full reproducibility (e.g., `xgboost==1.7.6`, `scikit-learn==1.2.2`, `numpy`, `pandas`).

---

### üìö Notes for the Host

* The pseudo-labeling strategy significantly improves performance on limited labeled data.
* Capping predictions between realistic business thresholds avoids leakage and overestimation.
* Ensemble strategy ensures stability across varying market/store behavior.
